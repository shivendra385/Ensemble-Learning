{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Can we use Bagging for regression problems?\n",
        "->Yes, bagging can be used for regression problems. Bagging regressors work by combining the predictions of multiple base regressors trained on different subsets of the data.\n",
        "\n",
        "2. What is the difference between multiple model training and single model training?\n",
        "-> Multiple model training involves training multiple models on the same data, whereas single model training involves training a single model. Ensemble methods combine the predictions of multiple models to improve performance.\n",
        "\n",
        "3. Explain the concept of feature randomness in Random Forest.\n",
        "-> Feature randomness in Random Forest involves selecting a random subset of features at each node to consider for splitting. This helps reduce overfitting and improves the model's robustness.\n",
        "\n",
        "4. What is OOB (Out-of-Bag) Score?\n",
        "-> The OOB score is an estimate of the model's performance on unseen data. It's calculated using the samples that are not included in the bootstrap sample used to train each base model.\n",
        "\n",
        "5. How can you measure the importance of features in a Random Forest model?\n",
        "-> Feature importance in Random Forest can be measured using the Gini importance or permutation importance. Gini importance calculates the decrease in impurity after splitting on a feature, while permutation importance calculates the decrease in model performance after permuting a feature.\n",
        "\n",
        "6. Explain the working principle of a Bagging Classifier\n",
        "-> A Bagging Classifier works by training multiple base classifiers on different subsets of the data and combining their predictions through voting.\n",
        "\n",
        "7. How do you evaluate a Bagging Classifierâ€™s performance?\n",
        "-> A Bagging Classifier's performance can be evaluated using metrics such as accuracy, precision, recall, and F1-score.\n",
        "\n",
        "8. How does a Bagging Regressor work?\n",
        "-> A Bagging Regressor works by training multiple base regressors on different subsets of the data and combining their predictions through averaging.\n",
        "\n",
        "9. What is the main advantage of ensemble techniques?\n",
        "-> The main advantage of ensemble techniques is that they can improve the performance and robustness of machine learning models by combining the predictions of multiple models.\n",
        "\n",
        "10. What is the main challenge of ensemble methods?\n",
        "-> The main challenge of ensemble methods is that they can be computationally expensive and require careful tuning of hyperparameters.\n",
        "\n",
        "11. Explain the key idea behind ensemble techniques.\n",
        "-> The key idea behind ensemble techniques is to combine the predictions of multiple models to improve performance and reduce overfitting.\n",
        "\n",
        "12. What is a Random Forest Classifier?\n",
        "-> A Random Forest Classifier is an ensemble learning method that combines multiple decision trees trained on different subsets of the data.\n",
        "\n",
        "13. What are the main types of ensemble techniques?\n",
        "-> The main types of ensemble techniques are:\n",
        " a) Bagging\n",
        " b) Boosting\n",
        " c) Stacking\n",
        "\n",
        "14. What is ensemble learning in machine learning?\n",
        "-> Ensemble learning is a machine learning approach that combines the predictions of multiple models to improve performance and robustness.\n",
        "\n",
        "15. When should we avoid using ensemble methods?\n",
        "-> nsemble methods may not be suitable for:\n",
        "\n",
        "a) Simple problems\n",
        "b) Small datasets\n",
        "c) Interpretability is crucial\n",
        "\n",
        "16. How does Bagging help in reducing overfitting?\n",
        "-> Bootstrap sampling is used in bagging to create different subsets of the data for training each base model.\n",
        "\n",
        "17. Why is Random Forest better than a single Decision Tree?\n",
        "-> Random Forest is generally better than a single decision tree because it:\n",
        "a) Reduces overfitting\n",
        "b) Improves robustness\n",
        "c) Handles high-dimensional data\n",
        "\n",
        "18. What is the role of bootstrap sampling in Bagging?\n",
        "-> Bootstrap sampling is used in bagging to create different subsets of the data for training each base model.\n",
        "\n",
        "19. What are some real-world applications of ensemble techniques?\n",
        "-> Ensemble techniques have many real-world applications, including:\n",
        "a) Image classification\n",
        "b) Natural language processing\n",
        "c) Recommendation systems\n",
        "\n",
        "20. What is the difference between Bagging and Boosting?\n",
        "-> Bagging and boosting are both ensemble techniques, but they differ in how they combine models:\n",
        "\n",
        "a) Bagging combines models in parallel\n",
        "b) Boosting combines models sequentially, with each model focusing on the errors of the previous model"
      ],
      "metadata": {
        "id": "75DxBOHzJSEo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "niQkes-3CCkR",
        "outputId": "a4fd11c4-ffe4-4935-a503-d9a4f28832bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "#1Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy2\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Bagging Classifier\n",
        "bagging_clf = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=10)\n",
        "\n",
        "# Train model\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = bagging_clf.predict(X_test)\n",
        "\n",
        "# Evaluate model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE)2\n",
        "\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate regression dataset\n",
        "X, y = make_regression(n_samples=100, n_features=5, random_state=42)\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Bagging Regressor\n",
        "bagging_reg = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=10)\n",
        "\n",
        "# Train model\n",
        "bagging_reg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = bagging_reg.predict(X_test)\n",
        "\n",
        "# Evaluate model performance using MSE\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZVXFM1hCQ7-",
        "outputId": "2f420aec-049c-4ba7-aefa-8aa88a56be18"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 5449.871513137511\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3 Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores2\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Load breast cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100)\n",
        "\n",
        "# Train model\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Print feature importance scores\n",
        "feature_importances = rf_clf.feature_importances_\n",
        "print(\"Feature Importances:\")\n",
        "for feature_name, importance in zip(breast_cancer.feature_names, feature_importances):\n",
        "    print(f\"{feature_name}: {importance:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWC4pC95CwYu",
        "outputId": "8395aece-afe5-493f-aa7e-784ad1297361"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "mean radius: 0.02\n",
            "mean texture: 0.02\n",
            "mean perimeter: 0.07\n",
            "mean area: 0.07\n",
            "mean smoothness: 0.01\n",
            "mean compactness: 0.01\n",
            "mean concavity: 0.04\n",
            "mean concave points: 0.13\n",
            "mean symmetry: 0.00\n",
            "mean fractal dimension: 0.01\n",
            "radius error: 0.01\n",
            "texture error: 0.00\n",
            "perimeter error: 0.01\n",
            "area error: 0.01\n",
            "smoothness error: 0.00\n",
            "compactness error: 0.01\n",
            "concavity error: 0.00\n",
            "concave points error: 0.01\n",
            "symmetry error: 0.00\n",
            "fractal dimension error: 0.00\n",
            "worst radius: 0.09\n",
            "worst texture: 0.03\n",
            "worst perimeter: 0.08\n",
            "worst area: 0.11\n",
            "worst smoothness: 0.01\n",
            "worst compactness: 0.01\n",
            "worst concavity: 0.05\n",
            "worst concave points: 0.14\n",
            "worst symmetry: 0.01\n",
            "worst fractal dimension: 0.01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4  Train a Random Forest Regressor and compare its performance with a single Decision Tree2\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate regression dataset\n",
        "X, y = make_regression(n_samples=100, n_features=5, random_state=42)\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Random Forest Regressor and Decision Tree Regressor\n",
        "rf_reg = RandomForestRegressor(n_estimators=100)\n",
        "dt_reg = DecisionTreeRegressor()\n",
        "\n",
        "# Train models\n",
        "rf_reg.fit(X_train, y_train)\n",
        "dt_reg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_rf = rf_reg.predict(X_test)\n",
        "y_pred_dt = dt_reg.predict(X_test)\n",
        "\n",
        "# Evaluate model performance using MSE\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "mse_dt = mean_squared_error(y_test, y_pred_dt)\n",
        "print(\"Random Forest MSE:\", mse_rf)\n",
        "print(\"Decision Tree MSE:\", mse_dt)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qh9rKcTMDI6p",
        "outputId": "7833d83b-cc4e-4345-de58-66c7d8ca5949"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest MSE: 6179.840669693039\n",
            "Decision Tree MSE: 17124.51625947589\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5 Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier2\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Load breast cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Random Forest Classifier with OOB score\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\n",
        "\n",
        "# Train model\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Print OOB score\n",
        "print(\"Out-of-Bag Score:\", rf_clf.oob_score_)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfZK37qJDN-Z",
        "outputId": "85461fe8-c822-447d-f7c9-e3a350f7c758"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Out-of-Bag Score: 0.9560439560439561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#6 Train a Bagging Classifier using SVM as a base estimator and print accuracy\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load breast cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifiers with different numbers of trees\n",
        "for n_estimators in [10, 50, 100]:\n",
        "    rf_clf = RandomForestClassifier(n_estimators=n_estimators)\n",
        "    rf_clf.fit(X_train, y_train)\n",
        "    y_pred = rf_clf.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Accuracy with {n_estimators} trees:\", accuracy)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQxbiZMhDcGh",
        "outputId": "f0284399-dbe3-4c50-99b2-3055e2461797"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with 10 trees: 0.956140350877193\n",
            "Accuracy with 50 trees: 0.9649122807017544\n",
            "Accuracy with 100 trees: 0.9649122807017544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7 Train a Random Forest Classifier with different numbers of trees and compare accuracy\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load breast cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Bagging Classifier with Logistic Regression base estimator\n",
        "bagging_clf = BaggingClassifier(estimator=LogisticRegression(), n_estimators=10)\n",
        "\n",
        "# Train model\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_pred_proba = bagging_clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluate model performance using AUC score\n",
        "auc = roc_auc_score(y_test, y_pred_proba)\n",
        "print(\"AUC Score:\", auc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9us7A7EEI53",
        "outputId": "a67a3401-f858-4798-dcee-0103f98e5dad"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC Score: 0.9970520799213888\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#8 Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load breast cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Bagging Classifier with Logistic Regression base estimator\n",
        "bagging_clf = BaggingClassifier(estimator=LogisticRegression(max_iter=1000), n_estimators=10)\n",
        "\n",
        "# Train model\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_pred_proba = bagging_clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluate model performance using AUC score\n",
        "auc = roc_auc_score(y_test, y_pred_proba)\n",
        "print(\"AUC Score:\", auc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhUHq92-ErQ-",
        "outputId": "f400abb3-a861-434b-d549-53c87d023e1b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC Score: 0.9983622666229939\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#9 Train a Random Forest Regressor and analyze feature importance scores2\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Generate regression dataset\n",
        "X, y = make_regression(n_samples=100, n_features=5, random_state=42)\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(n_estimators=100)\n",
        "\n",
        "# Train model\n",
        "rf_reg.fit(X_train, y_train)\n",
        "\n",
        "# Print feature importance scores\n",
        "feature_importances = rf_reg.feature_importances_\n",
        "print(\"Feature Importances:\")\n",
        "for i, importance in enumerate(feature_importances):\n",
        "    print(f\"Feature {i+1}: {importance:.2f}\")\n",
        "\n",
        "# Get top 3 most important features\n",
        "top_features = sorted(enumerate(feature_importances), key=lambda x: x[1], reverse=True)[:3]\n",
        "print(\"\\nTop 3 Most Important Features:\")\n",
        "for feature in top_features:\n",
        "    print(f\"Feature {feature[0]+1}: {feature[1]:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNzSRk8QFE-9",
        "outputId": "ffb93023-5c51-4c15-f577-508df86772c2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "Feature 1: 0.10\n",
            "Feature 2: 0.51\n",
            "Feature 3: 0.19\n",
            "Feature 4: 0.14\n",
            "Feature 5: 0.06\n",
            "\n",
            "Top 3 Most Important Features:\n",
            "Feature 2: 0.51\n",
            "Feature 3: 0.19\n",
            "Feature 4: 0.14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#10 2 Train an ensemble model using both Bagging and Random Forest and compare accuracy\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load breast cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Bagging Classifier and Random Forest Classifier\n",
        "bagging_clf = BaggingClassifier(n_estimators=10)\n",
        "rf_clf = RandomForestClassifier(n_estimators=100)\n",
        "\n",
        "# Train models\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_bagging = bagging_clf.predict(X_test)\n",
        "y_pred_rf = rf_clf.predict(X_test)\n",
        "\n",
        "# Evaluate model accuracy\n",
        "accuracy_bagging = accuracy_score(y_test, y_pred_bagging)\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "print(\"Bagging Accuracy:\", accuracy_bagging)\n",
        "print(\"Random Forest Accuracy:\", accuracy_rf)\n",
        "\n",
        "# Compare accuracy\n",
        "if accuracy_bagging > accuracy_rf:\n",
        "    print(\"Bagging performs better\")\n",
        "elif accuracy_rf > accuracy_bagging:\n",
        "    print(\"Random Forest performs better\")\n",
        "else:\n",
        "    print(\"Both models perform equally well\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxSrK8V9FkxU",
        "outputId": "b22a99ff-8f7c-45df-902f-0ff441a54746"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Accuracy: 0.9736842105263158\n",
            "Random Forest Accuracy: 0.9649122807017544\n",
            "Bagging performs better\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#11 = Train a Random Forest Classifier and tune hyperparameters using GridSearchCV=\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load breast cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [10, 50, 100],\n",
        "    'max_depth': [None, 5, 10],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Initialize Random Forest Classifier and GridSearchCV\n",
        "rf_clf = RandomForestClassifier()\n",
        "grid_search = GridSearchCV(rf_clf, param_grid, cv=5)\n",
        "\n",
        "# Perform grid search\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print best hyperparameters and accuracy\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "print(\"Best Accuracy:\", grid_search.best_score_)\n",
        "\n",
        "# Train model with best hyperparameters and evaluate on test set\n",
        "best_rf_clf = grid_search.best_estimator_\n",
        "best_rf_clf.fit(X_train, y_train)\n",
        "y_pred = best_rf_clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9Hd6c-6Fr-L",
        "outputId": "fc5cfd7d-8216-486a-fc0d-0be31392efe9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 10}\n",
            "Best Accuracy: 0.9626373626373625\n",
            "Test Accuracy: 0.9649122807017544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#12 = Train a Bagging Regressor with different numbers of base estimators and compare performance=\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate regression dataset\n",
        "X, y = make_regression(n_samples=100, n_features=5, random_state=42)\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Bagging Regressors with different numbers of base estimators\n",
        "for n_estimators in [10, 50, 100]:\n",
        "    bagging_reg = BaggingRegressor(n_estimators=n_estimators)\n",
        "    bagging_reg.fit(X_train, y_train)\n",
        "    y_pred = bagging_reg.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    print(f\"MSE with {n_estimators} estimators: {mse}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vv9T5sfGBDq",
        "outputId": "124fa4e1-4e2c-4db8-936c-901f07346949"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE with 10 estimators: 6460.356950488409\n",
            "MSE with 50 estimators: 6563.180777446716\n",
            "MSE with 100 estimators: 6024.401634990491\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#13 Train a Random Forest Classifier and analyze misclassified samples=\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Load breast cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "y_pred = rf_clf.predict(X_test)\n",
        "\n",
        "# Analyze misclassified samples\n",
        "misclassified_samples = X_test[y_test != y_pred]\n",
        "print(\"Number of Misclassified Samples:\", len(misclassified_samples))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "082-l8V5GONi",
        "outputId": "1f5869b3-d4d0-412a-ee66-0bdf982199e6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Misclassified Samples: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#14  Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier=\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load breast cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Bagging Classifier and Decision Tree Classifier\n",
        "bagging_clf = BaggingClassifier(n_estimators=10)\n",
        "dt_clf = DecisionTreeClassifier()\n",
        "\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "dt_clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred_bagging = bagging_clf.predict(X_test)\n",
        "y_pred_dt = dt_clf.predict(X_test)\n",
        "\n",
        "# Compare accuracy\n",
        "accuracy_bagging = accuracy_score(y_test, y_pred_bagging)\n",
        "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
        "print(\"Bagging Accuracy:\", accuracy_bagging)\n",
        "print(\"Decision Tree Accuracy:\", accuracy_dt)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xK10k37jGUZZ",
        "outputId": "114d0fa4-131f-4e70-a577-cdfdb273404a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Accuracy: 0.956140350877193\n",
            "Decision Tree Accuracy: 0.9473684210526315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#15 = Train a Random Forest Classifier and visualize the confusion matrix=\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Load breast cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "y_pred = rf_clf.predict(X_test)\n",
        "\n",
        "# Create confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, cmap='Blues')\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "2oV3Z5WDGaJR",
        "outputId": "2584fa67-9b2c-4909-efd6-50580e77d170"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#16 Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy=\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import svm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load breast cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define base estimators\n",
        "base_estimators = [\n",
        "    ('dt', DecisionTreeClassifier()),\n",
        "    ('svm', svm.SVC(probability=True)),\n",
        "    ('lr', LogisticRegression(max_iter=1000))\n",
        "]\n",
        "\n",
        "# Train Stacking Classifier\n",
        "stacking_clf = StackingClassifier(estimators=base_estimators, final_estimator=LogisticRegression(max_iter=1000))\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "y_pred = stacking_clf.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Stacking Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLViF0otGpK4",
        "outputId": "faf5c6d4-f398-4979-cf3d-712a7f53174f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacking Accuracy: 0.9649122807017544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#17 = Train a Random Forest Classifier and print the top 5 most important features\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Load breast cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importance scores\n",
        "feature_importances = rf_clf.feature_importances_\n",
        "\n",
        "# Print top 5 most important features\n",
        "top_features = sorted(zip(breast_cancer.feature_names, feature_importances), key=lambda x: x[1], reverse=True)[:5]\n",
        "for feature_name, importance in top_features:\n",
        "    print(f\"{feature_name}: {importance:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jj7Mn0VwGwxI",
        "outputId": "70d4a528-c999-4b7d-a35b-25d9c8d24810"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "worst perimeter: 0.16\n",
            "worst radius: 0.13\n",
            "mean concave points: 0.11\n",
            "worst area: 0.11\n",
            "worst concave points: 0.09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#18  Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Load breast cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Bagging Classifier\n",
        "bagging_clf = BaggingClassifier(n_estimators=10)\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "y_pred = bagging_clf.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gx9lERm7G4vA",
        "outputId": "5a4db66d-74c7-4c08-dc09-46e5b8b0cbcf"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.9714285714285714\n",
            "Recall: 0.9577464788732394\n",
            "F1-score: 0.9645390070921985\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#19 Train a Random Forest Classifier and analyze the effect of max_depth on accuracy\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load breast cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifiers with different max_depth\n",
        "for max_depth in [None, 5, 10, 20]:\n",
        "    rf_clf = RandomForestClassifier(n_estimators=100, max_depth=max_depth)\n",
        "    rf_clf.fit(X_train, y_train)\n",
        "    y_pred = rf_clf.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Accuracy with max_depth={max_depth}: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5sjv5SZG-13",
        "outputId": "cb5611ee-47c0-49ba-a472-7a567b8c9d55"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with max_depth=None: 0.9649122807017544\n",
            "Accuracy with max_depth=5: 0.9649122807017544\n",
            "Accuracy with max_depth=10: 0.956140350877193\n",
            "Accuracy with max_depth=20: 0.956140350877193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#20  Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare performance=\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate regression dataset\n",
        "X, y = make_regression(n_samples=100, n_features=5, random_state=42)\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Bagging Regressors with different base estimators\n",
        "for base_estimator in [DecisionTreeRegressor(), KNeighborsRegressor()]:\n",
        "    bagging_reg = BaggingRegressor(estimator=base_estimator, n_estimators=10)\n",
        "    bagging_reg.fit(X_train, y_train)\n",
        "    y_pred = bagging_reg.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    print(f\"MSE with {type(base_estimator).__name__}: {mse}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9sdSTMcHOwH",
        "outputId": "8235c7a4-ec17-46b0-db48-b6d97f3cedd4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE with DecisionTreeRegressor: 8828.293771274244\n",
            "MSE with KNeighborsRegressor: 2211.909439606324\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#21  Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load breast cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "y_pred_proba = rf_clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluate performance using ROC-AUC score\n",
        "auc = roc_auc_score(y_test, y_pred_proba)\n",
        "print(\"ROC-AUC Score:\", auc)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "\n",
        "# Load breast cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "\n",
        "# Train Bagging Classifier and evaluate performance using cross-validation\n",
        "bagging_clf = BaggingClassifier(n_estimators=10)\n",
        "scores = cross_val_score(bagging_clf, X, y, cv=5)\n",
        "print(\"Cross-Validation Scores:\", scores)\n",
        "print(\"Average Cross-Validation Score:\", scores.mean())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgJ1rDOTHXQH",
        "outputId": "dcf61d29-d086-4c16-d1ba-b3cb7c6c1ee7"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.995250573206682\n",
            "Cross-Validation Scores: [0.92982456 0.92982456 0.97368421 0.96491228 0.92920354]\n",
            "Average Cross-Validation Score: 0.9454898307716194\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#22 Train a Bagging Classifier and evaluate its performance using cross-validatio.\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "\n",
        "# Load breast cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "\n",
        "# Train Bagging Classifier and evaluate performance using cross-validation\n",
        "bagging_clf = BaggingClassifier(n_estimators=10)\n",
        "scores = cross_val_score(bagging_clf, X, y, cv=5)\n",
        "print(\"Cross-Validation Scores:\", scores)\n",
        "print(\"Average Cross-Validation Score:\", scores.mean())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwXAUyufHmfu",
        "outputId": "11811cec-cddf-49d8-a9ce-b14ff06c4a49"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-Validation Scores: [0.92982456 0.93859649 0.98245614 0.92982456 0.97345133]\n",
            "Average Cross-Validation Score: 0.9508306163639186\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#23 Train a Random Forest Classifier and plot the Precision-Recall curv\u0010\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import precision_recall_curve, auc\n",
        "\n",
        "# Load breast cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "y_pred_proba = rf_clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Plot precision-recall curve\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
        "auc_score = auc(recall, precision)\n",
        "plt.plot(recall, precision)\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title(f'Precision-Recall Curve (AUC={auc_score:.2f})')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "A6sDZNidH8DV",
        "outputId": "969f4406-ebe9-4c88-bc20-098cea52627a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#24  Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy=\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load breast cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define base estimators\n",
        "base_estimators = [\n",
        "    ('rf', RandomForestClassifier(n_estimators=100)),\n",
        "    ('lr', LogisticRegression(max_iter=1000))\n",
        "]\n",
        "\n",
        "# Train Stacking Classifier\n",
        "stacking_clf = StackingClassifier(estimators=base_estimators, final_estimator=LogisticRegression(max_iter=1000))\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "y_pred = stacking_clf.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Stacking Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VVk3OD2IBzd",
        "outputId": "439c6b04-e2df-4e6a-f068-fad340767b8b"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacking Accuracy: 0.9736842105263158\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#25 Train a Bagging Regressor with different levels of bootstrap samples and compare performance.\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate regression dataset\n",
        "X, y = make_regression(n_samples=100, n_features=5, random_state=42)\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Bagging Regressors with different levels of bootstrap samples\n",
        "for max_samples in [0.5, 0.8, 1.0]:\n",
        "    bagging_reg = BaggingRegressor(n_estimators=10, max_samples=max_samples)\n",
        "    bagging_reg.fit(X_train, y_train)\n",
        "    y_pred = bagging_reg.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    print(f\"MSE with max_samples={max_samples}: {mse}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azM6wa1YIK-s",
        "outputId": "3e166197-b7df-4797-9a84-b2a39336770c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE with max_samples=0.5: 7545.164616255094\n",
            "MSE with max_samples=0.8: 6199.186670619503\n",
            "MSE with max_samples=1.0: 6808.908486064991\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uI8JeBwaIUz0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}